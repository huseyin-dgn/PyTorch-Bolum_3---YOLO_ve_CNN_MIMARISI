{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5adf275b",
   "metadata": {},
   "source": [
    "# CNN Modeline Dataset Verme SÃ¼reci\n",
    "\n",
    "## 1. Dataset FormatÄ±nÄ± SeÃ§\n",
    "Roboflowâ€™dan dataset indirirken birkaÃ§ seÃ§enek Ã§Ä±kar:\n",
    "- **Object Detection iÃ§in (YOLO, COCO, Pascal VOC)** â†’ bounding boxâ€™lÄ± veriler.\n",
    "- **Classification iÃ§in (Image Classification)** â†’ kÄ±rpÄ±lmÄ±ÅŸ nesneler, klasÃ¶rlere ayrÄ±lmÄ±ÅŸ halde.\n",
    "\n",
    "ðŸ‘‰ Sen kÄ±rptÄ±ysan ve her gÃ¶rÃ¼ntÃ¼ tek sÄ±nÄ±f iÃ§eriyorsa â†’ **Classification dataset** mantÄ±klÄ± olur.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b8cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c0311",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # 1 kanal yap\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # 1 kanal normalize\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b1f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # 1 kanal yap\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 1 kanal normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root=r\"C:\\Users\\hdgn5\\OneDrive\\MasaÃ¼stÃ¼\\PyTorch CNN AnlatÄ±mlarÄ±\\Dataset\\- Brain TÃ¼mor\\Training\", transform=transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # <-- buraya ekle\n",
    "    train_transform\n",
    "]))\n",
    "tes_dataset = datasets.ImageFolder(root=r\"C:\\Users\\hdgn5\\OneDrive\\MasaÃ¼stÃ¼\\PyTorch CNN AnlatÄ±mlarÄ±\\Dataset\\- Brain TÃ¼mor\\Testing\", transform=transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # <-- buraya ekle\n",
    "    test_transform\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=32 , shuffle=True)\n",
    "test_loader = DataLoader(tes_dataset,batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b81815",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images , label in train_loader:\n",
    "    print(\"Batch : \" , images.shape )\n",
    "    print(\"Label : \" , label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85261fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_test = [label for _ , label in tes_dataset]\n",
    "# counts_txt = Counter(labels_test)\n",
    "\n",
    "# for idx , cls in enumerate(tes_dataset.classes):\n",
    "#     print(f\"{cls} : {counts_txt[idx]} Ã¶rnek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# 1 batch al\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# 16 Ã¶rneÄŸi grid olarak gÃ¶ster\n",
    "grid_img = make_grid(images[:16], nrow=4, normalize=True)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(grid_img.permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Batch boyutu: {images.size(0)}\")\n",
    "print(f\"Dataset boyutu (orijinal): {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff2aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeBlock(nn.Module):\n",
    "    def __init__(self, channels , reduction = 8 ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels,channels// reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction,channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        b,c,_,_ = x.size()\n",
    "        y = torch.mean(x,dim=(2,3))\n",
    "        y = F.silu(self.fc1(y))\n",
    "        y = torch.sigmoid(self.fc2(y)).view(b,c,1,1)\n",
    "        return x * y \n",
    "class MiniBottleNeckSe(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels , stride=1,use_projection = False , p_drop =0.05):\n",
    "        super().__init__()\n",
    "        mid_channels = max(1,out_channels // 4)\n",
    "        self.p_drop = p_drop\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        chanells = [in_channels , mid_channels , mid_channels ,out_channels]\n",
    "        kernel_sizes = [1,3,1]\n",
    "        strides = [1,stride,1]\n",
    "\n",
    "        for i in range(3):\n",
    "            self.layers.append(nn.BatchNorm2d(chanells[i]))\n",
    "            self.layers.append(nn.SiLU())\n",
    "            self.layers.append(nn.Conv2d(chanells[i] , chanells[i+1] , kernel_size=kernel_sizes[i] , stride=strides[i] , padding= 1 if kernel_sizes[i] == 3 else 0))\n",
    "\n",
    "        if use_projection or in_channels != out_channels or stride!=1:\n",
    "            self.shortcut = nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride)\n",
    "        else :\n",
    "            self.shortcut = nn.Identity()\n",
    "        \n",
    "        self.se = SeBlock(out_channels,reduction=8)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = x\n",
    "\n",
    "        for i in range(0,len(self.layers),3):\n",
    "            out = self.layers[i](out)\n",
    "            out = self.layers[i+1](out)\n",
    "            out = self.layers[i+2](out)\n",
    "\n",
    "        if self.training and torch.rand(1).item() < self.p_drop:\n",
    "            out = identity\n",
    "        \n",
    "        out = self.se(out)\n",
    "        out += identity\n",
    "        return out\n",
    "class MiniCNNSe(nn.Module):\n",
    "    def __init__(self, input_channels = 1, num_classes = 4 , conv_channels =[64,128,256] , r_b = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.res_block = nn.ModuleList()\n",
    "        in_ch = input_channels\n",
    "\n",
    "        for out_ch in conv_channels:\n",
    "            self.conv_layers.append(nn.Conv2d(in_ch,out_ch ,kernel_size=3 , stride=1 , padding=1))\n",
    "            self.conv_layers.append(nn.BatchNorm2d(out_ch))\n",
    "\n",
    "            for _ in range(r_b):\n",
    "                stride_val = 2 if in_ch != out_ch else 1 \n",
    "                use_prof = True if in_ch !=out_ch else False\n",
    "                self.res_block.append(MiniBottleNeckSe(out_ch ,out_ch , stride=stride_val , use_projection=use_prof))\n",
    "            \n",
    "            in_ch = out_ch\n",
    "\n",
    "            self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "            self.fc = nn.Linear(conv_channels[-1] ,  num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        res_idx = 0\n",
    "        for i in range(0,len(self.conv_layers),2):\n",
    "            x = F.silu(self.conv_layers[i+1](self.conv_layers[i](x)))\n",
    "            x = F.dropout2d(x,p=0.1 , training=self.training)\n",
    "\n",
    "            for _ in range(1):\n",
    "                x = self.res_block[res_idx](x)\n",
    "                res_idx += 1\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd0714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_dataset.classes) \n",
    "model = MiniCNNSe(input_channels=1, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76056d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(32,1,224,224), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d8c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fonk = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters() , lr=0.001 , weight_decay=1e-4 , betas=(0.9,0.999) , eps=1e-7)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer,mode=\"min\" , patience=5,factor=0.3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deaff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, delta=0, verbose=False, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.rest_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_wts = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            if self.rest_best_weights:\n",
    "                self.best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        elif val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.rest_best_weights:\n",
    "                self.best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if self.verbose:\n",
    "                print(f\"DoÄŸrulama kaybÄ± iyileÅŸti: {val_loss:.4f}. Modelin en iyi aÄŸÄ±rlÄ±klarÄ± kaydedildi.\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"DoÄŸrulama kaybÄ±nda iyileÅŸme yok. SayaÃ§: {self.counter}/{self.patience}\")\n",
    "            \n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"Erken durdurma tetiklendi. EÄŸitim durduruldu.\")\n",
    "\n",
    "    def restore_weights(self, model):\n",
    "        if self.rest_best_weights and self.best_model_wts is not None:\n",
    "            model.load_state_dict(self.best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=7, verbose=True, restore_best_weights=True)\n",
    "\n",
    "epochs_num = 10\n",
    "best_val_loss = float('inf')\n",
    "max_norm = 2\n",
    "\n",
    "for epoch in range(epochs_num):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs_num}\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fonk(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        correct_train += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = 100 * correct_train / len(train_dataset)\n",
    "\n",
    "     # EVAL\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_fonk(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            correct_val += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "    val_acc = 100 * correct_val / len(tes_dataset)\n",
    "\n",
    "    lr_scheduler.step(val_loss)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{epochs_num} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | \"\n",
    "          f\"Time: {epoch_time:.1f}s | \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "     # EarlyStopping Ã§aÄŸÄ±r\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    # Best model kaydet\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "early_stopping.restore_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4535414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeli eval moduna al\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Confusion Matrix oluÅŸtur\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=tes_dataset.classes)\n",
    "\n",
    "# CM'yi gÃ¶rselleÅŸtir\n",
    "plt.figure(figsize=(8,8))\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "plt.title(\"Test Seti Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# SÄ±nÄ±f raporu\n",
    "report = classification_report(all_labels, all_preds, target_names=tes_dataset.classes)\n",
    "print(\"Classification Report:\\n\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f471b9",
   "metadata": {},
   "source": [
    "# Bilgisayarla GÃ¶rÃ¼ Projesi Genel Ä°ÅŸ AkÄ±ÅŸÄ±\n",
    "\n",
    "## 1. Veri Toplama\n",
    "- Kameradan, internetten, aÃ§Ä±k datasetlerden veya kendi Ã§ektiÄŸin gÃ¶rÃ¼ntÃ¼lerden veri toplarsÄ±n.  \n",
    "- Veri Ã§eÅŸitliliÄŸi Ã¶nemlidir (farklÄ± Ä±ÅŸÄ±k, aÃ§Ä±, ortam).\n",
    "\n",
    "\n",
    "\n",
    "## 2. Veri Etiketleme\n",
    "- Roboflow, LabelImg, CVAT gibi araÃ§larla nesneleri iÅŸaretlersin.  \n",
    "- Ä°ki ana senaryo vardÄ±r:\n",
    "  - **Image Classification (SÄ±nÄ±flandÄ±rma):** Her resim tek bir sÄ±nÄ±fa ait (Ã¶rn. â€œkediâ€ veya â€œkÃ¶pekâ€).  \n",
    "  - **Object Detection (Nesne Tespiti):** Resimde birden Ã§ok nesne olabilir, bounding box ile iÅŸaretlenir (Ã¶rn. yol, tÃ¼msek, araba).  \n",
    "  - **Segmentation (BÃ¶lÃ¼tleme):** Piksel bazlÄ± iÅŸaretleme (Ã¶rn. tÃ¼m yol pikselleri).  \n",
    "\n",
    "\n",
    "\n",
    "## 3. Veri Ã–n Ä°ÅŸleme (Preprocessing)\n",
    "- GÃ¶rÃ¼ntÃ¼leri boyutlandÄ±rma, normalize etme, gÃ¼rÃ¼ltÃ¼ temizleme.  \n",
    "- Augmentation: DÃ¶ndÃ¼rme, kÄ±rpma, parlaklÄ±k deÄŸiÅŸtirme â†’ modelin genelleme kabiliyetini artÄ±rÄ±r.  \n",
    "\n",
    "\n",
    "## 4. Dataset HazÄ±rlama\n",
    "- Veriyi **train / validation / test** olarak ayÄ±r:  \n",
    "  - %70 â†’ EÄŸitim  \n",
    "  - %20 â†’ DoÄŸrulama  \n",
    "  - %10 â†’ Test  \n",
    "\n",
    "\n",
    "## 5. Model SeÃ§imi\n",
    "- **CNN tabanlÄ± modeller:** Image classification iÃ§in uygundur.  \n",
    "- **YOLO, Faster R-CNN, SSD:** Object detection iÃ§in.  \n",
    "- **U-Net, Mask R-CNN:** Segmentation iÃ§in.  \n",
    "\n",
    "\n",
    "## 6. Modelin EÄŸitilmesi\n",
    "- Framework seÃ§: **PyTorch, TensorFlow, Keras, Ultralytics YOLO**.  \n",
    "- Hyperparametreler: batch size, learning rate, epoch sayÄ±sÄ±.  \n",
    "- GPU/TPU varsa hÄ±z kazanÄ±rsÄ±n.  \n",
    "\n",
    "\n",
    "## 7. Modelin DeÄŸerlendirilmesi\n",
    "- **Accuracy, Precision, Recall, F1-score, mAP** gibi metriklerle baÅŸarÄ± Ã¶lÃ§Ã¼lÃ¼r.  \n",
    "- Overfitting var mÄ± diye validation lossâ€™a bakÄ±lÄ±r.  \n",
    "\n",
    "\n",
    "## 8. Modelin Ä°yileÅŸtirilmesi\n",
    "- Daha fazla veri topla.  \n",
    "- Augmentation Ã§eÅŸitliliÄŸini artÄ±r.  \n",
    "- Daha gÃ¼Ã§lÃ¼ bir model mimarisi dene.  \n",
    "- Transfer learning kullan (Ã¶nceden eÄŸitilmiÅŸ aÄŸÄ±rlÄ±klar).  \n",
    "\n",
    "\n",
    "\n",
    "## 9. Modelin KullanÄ±ma AlÄ±nmasÄ± (Deployment)\n",
    "- EÄŸitimden sonra model ÅŸu ortamlara alÄ±nabilir:\n",
    "  - **Edge cihaz:** Raspberry Pi, Jetson Nano (gÃ¶mÃ¼lÃ¼ sistemler).  \n",
    "  - **Mobil uygulama:** TensorFlow Lite, CoreML.  \n",
    "  - **Sunucu & API:** Flask, FastAPI, Roboflow API.  \n",
    "\n",
    "## 10. GerÃ§ek ZamanlÄ± KullanÄ±m\n",
    "- Kamera akÄ±ÅŸÄ±nÄ± al â†’ modele gÃ¶nder â†’ Ã§Ä±ktÄ± (Ã¶r. sesli uyarÄ±, gÃ¶rsel iÅŸaret, API cevabÄ±).  \n",
    "- UygulamanÄ±n asÄ±l deÄŸer kattÄ±ÄŸÄ± kÄ±sÄ±m burasÄ±dÄ±r.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044ea25",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
