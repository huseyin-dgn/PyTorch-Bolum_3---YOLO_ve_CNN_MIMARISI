{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5adf275b",
   "metadata": {},
   "source": [
    "# CNN Modeline Dataset Verme Süreci\n",
    "\n",
    "## 1. Dataset Formatını Seç\n",
    "Roboflow’dan dataset indirirken birkaç seçenek çıkar:\n",
    "- **Object Detection için (YOLO, COCO, Pascal VOC)** → bounding box’lı veriler.\n",
    "- **Classification için (Image Classification)** → kırpılmış nesneler, klasörlere ayrılmış halde.\n",
    "\n",
    "👉 Sen kırptıysan ve her görüntü tek sınıf içeriyorsa → **Classification dataset** mantıklı olur.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b8cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c0311",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # 1 kanal yap\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # 1 kanal normalize\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b1f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # 1 kanal yap\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 1 kanal normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root=r\"C:\\Users\\hdgn5\\OneDrive\\Masaüstü\\PyTorch CNN Anlatımları\\Dataset\\- Brain Tümor\\Training\", transform=transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # <-- buraya ekle\n",
    "    train_transform\n",
    "]))\n",
    "tes_dataset = datasets.ImageFolder(root=r\"C:\\Users\\hdgn5\\OneDrive\\Masaüstü\\PyTorch CNN Anlatımları\\Dataset\\- Brain Tümor\\Testing\", transform=transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # <-- buraya ekle\n",
    "    test_transform\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=32 , shuffle=True)\n",
    "test_loader = DataLoader(tes_dataset,batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b81815",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images , label in train_loader:\n",
    "    print(\"Batch : \" , images.shape )\n",
    "    print(\"Label : \" , label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85261fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_test = [label for _ , label in tes_dataset]\n",
    "# counts_txt = Counter(labels_test)\n",
    "\n",
    "# for idx , cls in enumerate(tes_dataset.classes):\n",
    "#     print(f\"{cls} : {counts_txt[idx]} örnek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# 1 batch al\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# 16 örneği grid olarak göster\n",
    "grid_img = make_grid(images[:16], nrow=4, normalize=True)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(grid_img.permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Batch boyutu: {images.size(0)}\")\n",
    "print(f\"Dataset boyutu (orijinal): {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff2aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeBlock(nn.Module):\n",
    "    def __init__(self, channels , reduction = 8 ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(channels,channels// reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction,channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        b,c,_,_ = x.size()\n",
    "        y = torch.mean(x,dim=(2,3))\n",
    "        y = F.silu(self.fc1(y))\n",
    "        y = torch.sigmoid(self.fc2(y)).view(b,c,1,1)\n",
    "        return x * y \n",
    "class MiniBottleNeckSe(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels , stride=1,use_projection = False , p_drop =0.05):\n",
    "        super().__init__()\n",
    "        mid_channels = max(1,out_channels // 4)\n",
    "        self.p_drop = p_drop\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        chanells = [in_channels , mid_channels , mid_channels ,out_channels]\n",
    "        kernel_sizes = [1,3,1]\n",
    "        strides = [1,stride,1]\n",
    "\n",
    "        for i in range(3):\n",
    "            self.layers.append(nn.BatchNorm2d(chanells[i]))\n",
    "            self.layers.append(nn.SiLU())\n",
    "            self.layers.append(nn.Conv2d(chanells[i] , chanells[i+1] , kernel_size=kernel_sizes[i] , stride=strides[i] , padding= 1 if kernel_sizes[i] == 3 else 0))\n",
    "\n",
    "        if use_projection or in_channels != out_channels or stride!=1:\n",
    "            self.shortcut = nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride)\n",
    "        else :\n",
    "            self.shortcut = nn.Identity()\n",
    "        \n",
    "        self.se = SeBlock(out_channels,reduction=8)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = x\n",
    "\n",
    "        for i in range(0,len(self.layers),3):\n",
    "            out = self.layers[i](out)\n",
    "            out = self.layers[i+1](out)\n",
    "            out = self.layers[i+2](out)\n",
    "\n",
    "        if self.training and torch.rand(1).item() < self.p_drop:\n",
    "            out = identity\n",
    "        \n",
    "        out = self.se(out)\n",
    "        out += identity\n",
    "        return out\n",
    "class MiniCNNSe(nn.Module):\n",
    "    def __init__(self, input_channels = 1, num_classes = 4 , conv_channels =[64,128,256] , r_b = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.res_block = nn.ModuleList()\n",
    "        in_ch = input_channels\n",
    "\n",
    "        for out_ch in conv_channels:\n",
    "            self.conv_layers.append(nn.Conv2d(in_ch,out_ch ,kernel_size=3 , stride=1 , padding=1))\n",
    "            self.conv_layers.append(nn.BatchNorm2d(out_ch))\n",
    "\n",
    "            for _ in range(r_b):\n",
    "                stride_val = 2 if in_ch != out_ch else 1 \n",
    "                use_prof = True if in_ch !=out_ch else False\n",
    "                self.res_block.append(MiniBottleNeckSe(out_ch ,out_ch , stride=stride_val , use_projection=use_prof))\n",
    "            \n",
    "            in_ch = out_ch\n",
    "\n",
    "            self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "            self.fc = nn.Linear(conv_channels[-1] ,  num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        res_idx = 0\n",
    "        for i in range(0,len(self.conv_layers),2):\n",
    "            x = F.silu(self.conv_layers[i+1](self.conv_layers[i](x)))\n",
    "            x = F.dropout2d(x,p=0.1 , training=self.training)\n",
    "\n",
    "            for _ in range(1):\n",
    "                x = self.res_block[res_idx](x)\n",
    "                res_idx += 1\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd0714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_dataset.classes) \n",
    "model = MiniCNNSe(input_channels=1, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76056d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(32,1,224,224), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d8c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fonk = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters() , lr=0.001 , weight_decay=1e-4 , betas=(0.9,0.999) , eps=1e-7)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer,mode=\"min\" , patience=5,factor=0.3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deaff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, delta=0, verbose=False, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.rest_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_wts = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            if self.rest_best_weights:\n",
    "                self.best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        elif val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.rest_best_weights:\n",
    "                self.best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if self.verbose:\n",
    "                print(f\"Doğrulama kaybı iyileşti: {val_loss:.4f}. Modelin en iyi ağırlıkları kaydedildi.\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"Doğrulama kaybında iyileşme yok. Sayaç: {self.counter}/{self.patience}\")\n",
    "            \n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"Erken durdurma tetiklendi. Eğitim durduruldu.\")\n",
    "\n",
    "    def restore_weights(self, model):\n",
    "        if self.rest_best_weights and self.best_model_wts is not None:\n",
    "            model.load_state_dict(self.best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=7, verbose=True, restore_best_weights=True)\n",
    "\n",
    "epochs_num = 10\n",
    "best_val_loss = float('inf')\n",
    "max_norm = 2\n",
    "\n",
    "for epoch in range(epochs_num):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs_num}\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fonk(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        correct_train += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = 100 * correct_train / len(train_dataset)\n",
    "\n",
    "     # EVAL\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_fonk(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            correct_val += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "    val_acc = 100 * correct_val / len(tes_dataset)\n",
    "\n",
    "    lr_scheduler.step(val_loss)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{epochs_num} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | \"\n",
    "          f\"Time: {epoch_time:.1f}s | \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "     # EarlyStopping çağır\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    # Best model kaydet\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "early_stopping.restore_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4535414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeli eval moduna al\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Confusion Matrix oluştur\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=tes_dataset.classes)\n",
    "\n",
    "# CM'yi görselleştir\n",
    "plt.figure(figsize=(8,8))\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "plt.title(\"Test Seti Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Sınıf raporu\n",
    "report = classification_report(all_labels, all_preds, target_names=tes_dataset.classes)\n",
    "print(\"Classification Report:\\n\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f471b9",
   "metadata": {},
   "source": [
    "# Bilgisayarla Görü Projesi Genel İş Akışı\n",
    "\n",
    "## 1. Veri Toplama\n",
    "- Kameradan, internetten, açık datasetlerden veya kendi çektiğin görüntülerden veri toplarsın.  \n",
    "- Veri çeşitliliği önemlidir (farklı ışık, açı, ortam).\n",
    "\n",
    "\n",
    "\n",
    "## 2. Veri Etiketleme\n",
    "- Roboflow, LabelImg, CVAT gibi araçlarla nesneleri işaretlersin.  \n",
    "- İki ana senaryo vardır:\n",
    "  - **Image Classification (Sınıflandırma):** Her resim tek bir sınıfa ait (örn. “kedi” veya “köpek”).  \n",
    "  - **Object Detection (Nesne Tespiti):** Resimde birden çok nesne olabilir, bounding box ile işaretlenir (örn. yol, tümsek, araba).  \n",
    "  - **Segmentation (Bölütleme):** Piksel bazlı işaretleme (örn. tüm yol pikselleri).  \n",
    "\n",
    "\n",
    "\n",
    "## 3. Veri Ön İşleme (Preprocessing)\n",
    "- Görüntüleri boyutlandırma, normalize etme, gürültü temizleme.  \n",
    "- Augmentation: Döndürme, kırpma, parlaklık değiştirme → modelin genelleme kabiliyetini artırır.  \n",
    "\n",
    "\n",
    "## 4. Dataset Hazırlama\n",
    "- Veriyi **train / validation / test** olarak ayır:  \n",
    "  - %70 → Eğitim  \n",
    "  - %20 → Doğrulama  \n",
    "  - %10 → Test  \n",
    "\n",
    "\n",
    "## 5. Model Seçimi\n",
    "- **CNN tabanlı modeller:** Image classification için uygundur.  \n",
    "- **YOLO, Faster R-CNN, SSD:** Object detection için.  \n",
    "- **U-Net, Mask R-CNN:** Segmentation için.  \n",
    "\n",
    "\n",
    "## 6. Modelin Eğitilmesi\n",
    "- Framework seç: **PyTorch, TensorFlow, Keras, Ultralytics YOLO**.  \n",
    "- Hyperparametreler: batch size, learning rate, epoch sayısı.  \n",
    "- GPU/TPU varsa hız kazanırsın.  \n",
    "\n",
    "\n",
    "## 7. Modelin Değerlendirilmesi\n",
    "- **Accuracy, Precision, Recall, F1-score, mAP** gibi metriklerle başarı ölçülür.  \n",
    "- Overfitting var mı diye validation loss’a bakılır.  \n",
    "\n",
    "\n",
    "## 8. Modelin İyileştirilmesi\n",
    "- Daha fazla veri topla.  \n",
    "- Augmentation çeşitliliğini artır.  \n",
    "- Daha güçlü bir model mimarisi dene.  \n",
    "- Transfer learning kullan (önceden eğitilmiş ağırlıklar).  \n",
    "\n",
    "\n",
    "\n",
    "## 9. Modelin Kullanıma Alınması (Deployment)\n",
    "- Eğitimden sonra model şu ortamlara alınabilir:\n",
    "  - **Edge cihaz:** Raspberry Pi, Jetson Nano (gömülü sistemler).  \n",
    "  - **Mobil uygulama:** TensorFlow Lite, CoreML.  \n",
    "  - **Sunucu & API:** Flask, FastAPI, Roboflow API.  \n",
    "\n",
    "## 10. Gerçek Zamanlı Kullanım\n",
    "- Kamera akışını al → modele gönder → çıktı (ör. sesli uyarı, görsel işaret, API cevabı).  \n",
    "- Uygulamanın asıl değer kattığı kısım burasıdır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044ea25",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
